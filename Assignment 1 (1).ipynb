{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Understanding the Literature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A Mathematical Theory of Communication  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinaldo - 12981736"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report will discuss the literature namely the ‘A Mathematical Theory of Communication’ which is written by Claude E. Shannon. Claude Shannon's numerical speculation of the correspondence concerns quantitative limits of interceded correspondence. The theory has a history in cryptography and of evaluating telephone traffic. Paralleling work by U.S. cybernetician Norbert Wiener and Soviet realist Andrei N. Kolmogorov, the speculation was first dispersed after declassification in 1948. On account of Wilbur Schramm's drive, it appeared in 1949 as a book with a succinct evaluate by Warren Weaver who is widely known as an advocate for science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This literature discusses the variation of methods in modulation specifically in PCM (Pulse code modulation) and PPM (Pulse Position Modulation) which is an exchange of bandwidth that correlates the general theory of communication. This literature is presenting an extended version of theory done by Nyquist and Hartley which inclu!des a new number of factors, which is the effect of noise in sending a message, as well as the statistical structure of the message with the condition of the receiver of the message. This paper consists of 22 theorem and 7 appendices. This literature extends the use of logarithmic measure due to the fact that information is transmitted on a value of a binary digit which requires a base of number 2 and most measurements are done with the linear equation as a matter of comparison with the variables that they got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/ReinZKriZ/UTS_ML2019_ID12981736/master/dsfas.PNG\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://raw.githubusercontent.com/ReinZKriZ/UTS_ML2019_ID12981736/master/dsfas.PNG\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 : Diagram of General Communication System (Shannon, 1948)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This literature pointed out that the main problem with communication is whenever a message is sent the message will be reproduced at one point either the source of the message or the destination of the message.  The main communication system that this paper discussing is the schematic design of a system which consists of 5 main parts, the information source, transmitter, channel, receiver, and the destination. This paper also classified the communication systems into 3 categories, firstly discrete where the message and the signal are a sequence of discrete symbols, secondly the continuous system where the message and signal are treated as a continuous function. Thirdly, mixed system, where there will be discrete and continuous variable is used. This Literature laid out all the basic components of the theory of communications, from how a message source produces a message, to how the receiver of the message gets its message. This paper also introduced the concepts of information entropy, where it is the average rate of information produced at any determined process which mainly involving the exchange of information. The unit of expression for this information is called the binary unit/bit (H) where p is the probability of the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/ReinZKriZ/UTS_ML2019_ID12981736/master/Capture3.PNG\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://raw.githubusercontent.com/ReinZKriZ/UTS_ML2019_ID12981736/master/Capture3.PNG\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2 : Graph plotting the H bits and probabilities (Shannon, 1948)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper also introduces the redundancy of a communication channel, where it is the limit difference between the capacity of how much information could hold in the channel and how much of it is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ C = MaxH(y)-H(n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Redundancy theorem could be very useful to test out an unused channel capacity, or even to duplicate transmission of message. Redundancy seems to be very wasteful but not only in the early year even now it is very important to calculate cost and space of message through a channel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Innovation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper presented a new way of asking a question, where he uses Markov models as the basis for how we can think about communication. He proves that the English language has its statistical dependencies. This paper uses the Markov’s chain to generate a similar-looking text and even predict an outcome of the next group of the letter or even the next group of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/ReinZKriZ/UTS_ML2019_ID12981736/master/Capture2.PNG\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://raw.githubusercontent.com/ReinZKriZ/UTS_ML2019_ID12981736/master/Capture2.PNG\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2 : Example of the system frequency transition on a graph (Shannon, 1948)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory ‘Information Entropy’ presented by this paper introduced a theory called the first order of approximation, this will get a very closed outcome to the one provided. A very outstanding application of this theorem is the series of approximation in English, by creating a group of letters will help to determine the efficiency of the next outcome. This will deduce more information into a very distinct and detail manner. The second order of approximation will help develop the sequence which makes the information makes more sense. The order of approximation could be extended further to third-order approximation until it could develop the whole passage where statistical dependencies will help develop the structure.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper presented the deduced a comprehensive theory of Channel capacity where it solidity the Nyquist rate which is the number of pulses that could be placed in the channel and Hartley’s Law in formulating a way to quantify information and it’s data signalling rate. This paper poses a good amount of deal to address the problem measuring communication that is measured and addressed by a discrete type of system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although some theorem presented by this paper is somewhat an extension of other mathematician, however there is no doubt other thoughtful and innovative theorem originally written by Shannon is highly appreciated in many connected fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technical quality presented in the paper is extremely very descriptive with the relevant steps of proving each and every variable in the theorem. The theorem proposed by Shannon is somehow very true to the fact that Bandwith and Channel capacity are practically proportional, the higher the bandwidth the more capacity of information could flow through the channel. The other variable such as noise (N) and signal strength/message power (Q) or mostly known as (S) is the only variable that inconsistent where higher noise will have no channel capacity or higher message power will result in more channel capacity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ R = W^{}_{1} log\\frac{Q}{N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper also shows how to compute the channel capacity with two conditions, when the data signalling rate is lower than its capacity or higher than its capacity rate. The paper also proves that the growth of blocks of symbols under numbers of circumstances, for instance, the finite state condition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W^{}_{1} log\\frac{Q^{}_{1}}{N} \\leq R \\leq W^{}_{1} log\\frac{Q}{N} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Information Entropy mainly focusing on the length of time or the efficiency of how the message is produced. This paper produces a good example of how a probability tree could be decomposed, the understanding of what these theorem for is the efficiency of how a message is created over the channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(x) = -K \\sum^{n}_{i=1}\\mathop{}_{\\mkern-5mu x\\in X} p^{}{x}log^{}_{2} p^{}{x} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://raw.githubusercontent.com/ReinZKriZ/UTS_ML2019_ID12981736/master/tree.PNG\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://raw.githubusercontent.com/ReinZKriZ/UTS_ML2019_ID12981736/master/tree.PNG\", width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencing on figure 3, He also provide the range of H if the condition of the message is more than 0 or less than 0, The negative sign is to make sure that the entropies are positive quantities. \n",
    "$$ 0 \\leq H(x) \\leq log^{}_{2}N^{}{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application and X-factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this theorem is developed in the early nineties, lots and lots of development worked has done over the application of this theorem. Simple example yet very widely used in the world is that, whenever a wireless gadget is installed over the channel, this theorem is widely used to calculate the amount of information or data that could flow through the channel. The very unexpected turn of application of this paper theorem is the application of data streams and sparse recovery in the field of computer science (Price and Woodruff, 2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very undoubtedly that this paperwork we can encounter them every day in our life, whenever we are buying a laptop or a computer, we need to know what is the memory capacity and speed, or when registering for an internet service at home,we need to make sure that the provider has high transmission rate, or even attaching a file on email we need to check the size of our file. However some other interesting field that this paper could relate to in the field of thermodynamics where we definitely know the second law of thermodynamics is that for any states of closed system, energy like pressure or temperature will always be less than its initial state (Laws of Thermodynamics, 2019). Where only external materials may counteract the system, Shannon’s theory of communication has been considered as a general foundation of this law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I believe the presentation of this paper is less organized, for instance, the proof of concept for each the theorem are presented in the appendix which is very confusing at first. The categorization of this paper could have a better arrangement when stating the type of system used in the paper at the very first paragraph which causes confusion for its reader. However, this paper has presented the details and work of how every equation in a very detail manner. Perhaps with better and clear navigation of the paper will definitely help the future reader. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Price, E. and Woodruff, D. 2012, Applications of the Shannon-Hartley theorem to data streams and sparse recovery, 2012 IEEE International Symposium on Information Theory Proceedings,."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LAWS OF THERMODYNAMICS 2019, Www2.estrellamountain.edu. viewed 23 August 2019, <https://www2.estrellamountain.edu/faculty/farabee/biobk/BioBookEner1.html>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Shannon, C. 1948, A Mathematical Theory of Communication, Bell System Technical Journal, vol 27, no 3, pp.379-423,."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft and Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Apply the Entropy on Sender, Receiver , I believe we can measure 3 basic entropies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Uncertainty of messages a at sender S , with a probability of x\n",
    "$$ H(S) = - \\sum \\mathop{}_{\\mkern-5mu a\\in S} p^{}{x}log^{}_{2} p^{}{x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Uncertainty of messages b at receiver R , with a probability of y\n",
    "$$ H(S) = - \\sum \\mathop{}_{\\mkern-5mu b\\in R} p^{}{y}log^{}_{2} p^{}{y} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Total Uncertainty of S and R pair , with a probability of xy\n",
    "$$ H(S) = - \\sum \\mathop{}_{\\mkern-5mu a\\in S}\\sum \\mathop{}_{\\mkern-5mu b\\in R} p^{}{xy}log^{}_{2} p^{}{xy} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
